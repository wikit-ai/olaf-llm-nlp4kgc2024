import os

from ecologits import EcoLogits
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage
import spacy

from olaf import Pipeline
from olaf.commons.errors import MissingEnvironmentVariable
from olaf.commons.llm_tools import LLMGenerator
from olaf.commons.logging_config import logger
from olaf.pipeline.pipeline_component.axiom_extraction import LLMBasedOWLAxiomExtraction
from olaf.pipeline.pipeline_component.candidate_term_enrichment import LLMBasedTermEnrichment
from olaf.pipeline.pipeline_component.concept_relation_extraction import(
    LLMBasedConceptExtraction,
    LLMBasedRelationExtraction
)
from olaf.pipeline.pipeline_component.concept_relation_hierarchy import LLMBasedHierarchisation
from olaf.pipeline.pipeline_component.term_extraction import LLMTermExtraction
from olaf.repository.corpus_loader import TextCorpusLoader
from olaf.repository.serialiser import KRJSONSerialiser


class CustomLLMGenerator(LLMGenerator):
    """Text generator based on the Mistral 7B model."""

    def check_resources(self) -> None:
        """Check that the resources needed to use the custom generator are available."""
        if "MISTRAL_API_KEY" not in os.environ:
            raise MissingEnvironmentVariable(self.__class__, "MISTRAL_API_KEY")

    def generate_text(self, prompt: list[dict[str, str]]) -> str:
        """Generate text based on a chat completion prompt for the Mistral 7B model.

        Parameters
        ----------
        prompt: list[dict[str, str]]
            The prompt to use for text generation.

        Returns
        -------
        str
            The output generated by the LLM.
        """
        EcoLogits.init()

        client = MistralClient(api_key=os.getenv("MISTRAL_API_KEY"))
        llm_output = ""
        try:
            response = client.chat(
                model="open-mistral-7b",
                temperature=0.0,
                messages=prompt
            )
            llm_output = response.choices[0].message.content
        except Exception as e:
            logger.error(
                """Exception %s still occurred after retries on Mistral API.
                         Skipping document %s...""",
                e,
                prompt[-1],
            )

        print(f"Energy consumption: {response.impacts.energy.value} kWh")
        print(f"GHG emissions: {response.impacts.gwp.value} kgCO2eq")

        return llm_output


def create_pipeline() -> Pipeline:
    """Initialise a pipeline.

    Returns
    -------
    Pipeline
        The new pipeline created.
    """
    spacy_model = spacy.load("en_core_web_lg")
    corpus_loader = TextCorpusLoader(
        # corpus_path=os.path.join(os.getenv('DATA_PATH'), "pizza_description.txt")
        corpus_path=os.path.join(os.getenv('DATA_PATH'), "defect_detection_description.txt")
    )
    pipeline = Pipeline(
        spacy_model=spacy_model,
        corpus_loader=corpus_loader
    )
    return pipeline


def mistral_prompt_concept_term_extraction(context: str) -> list[ChatMessage]:
    """Prompt template for concept term extraction with Mistral 7B model.

    Parameters
    ----------
    context: str
        The context to add in the prompt template.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content=f'Extract the most meaningful keywords of the following text. Keep only keywords that could be concepts and not relations. Write them as a python list of string with double quotes.\n\n###\nHere is an example. Text: This python package is about ontology learning. I do not know a lot about this field.\n["python package", "ontology learning", "field"]\n###\n\n# Text: {context}'
        )
    ]
    return prompt


def mistral_prompt_relation_term_extraction(context: str) -> list[ChatMessage]:
    """Prompt template for relation term extraction with Mistral 7B model.

    Parameters
    ----------
    context: str
        The context to add in the prompt template.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content=f'Extract the most meaningful words describing actions or states in the following text. Keep only words that could be relations and not concepts. Write them as a python list of string with double quotes.\n\n###\nHere is an example. Text: I plan to eat pizza tonight. I am looking for advice.\n["plan", "eat", "looking for"]\n###\n\n# Text: {context}'
        )
    ]
    return prompt


def mistral_prompt_term_enrichment(context: str) -> list[ChatMessage]:
    """Prompt template for term enrichment with Mistral 7B model.

    Parameters
    ----------
    context: str
        The context to add in the prompt template.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content='Give synonyms, hypernyms, hyponyms and antonyms of the following term. The output should be in json format with "synonyms", "hypernyms", "hyponyms" and "antonyms" as keys and a list a string as values. Return only the json, no explanation.\n\n###\nHere is an example. Term : dog{"synonyms": ["hound", "mutt"], "hypernyms":["animal", "mammal", "canine"], "hyponyms": ["labrador", "dalmatian"],"antonyms": []}\n###'
        ),
        ChatMessage(
            role="user",
            content=f"Term: {context}"
        )
    ]
    return prompt


def mistral_prompt_concept_extraction(doc_context: str, ct_labels: str) -> list[ChatMessage]:
    """Prompt template for concept extraction with Mistral 7B model.

    Parameters
    ----------
    doc_context: str
        Extract of document contents to use as context.
    ct_labels: str
        The candidate terms to group into concepts.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content=f'Based on the context given, group together the words listed below with the same meaning. Each group should correspond to one concept. The result should be given as a python list of list of string with double quotes. All words should be used.\n\n###\nHere is the output format: [["concept1_word1","concept1,word2"],["concept2_word1","concept2_word2","concept2_word3"]]\n###\n\n# Context: {doc_context}\n\n# Words : {ct_labels}'
        )
    ]
    return prompt


def mistral_prompt_relation_extraction(doc_context: str, ct_labels: str) -> list[ChatMessage]:
    """Prompt template for relation extraction with Mistral 7B model.

    Parameters
    ----------
    doc_context: str
        Extract of document contents to use as context.
    ct_labels: str
        The candidate terms to group as relations.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content=f'Based on the context given, group together the words listed below where each group should express the same relation. The result should be given as a python list of list of string with double quotes.\n\n###\nHere is the output format: [["concept1_word1","concept1,word2"],["concept2_word1","concept2_word2","concept2_word3"]]\n###\n\n# Context: {doc_context}\n\n# Words : {ct_labels}'
        )
    ]
    return prompt


def mistral_prompt_hierarchisation(doc_context: str, concepts_description: str) -> list[ChatMessage]:
    """Prompt template for hierarchisation with Mistral 7B model.

    Parameters
    ----------
    doc_context: str
        Extract of document contents where concepts appear to use as context.
    concepts_description: str
        Textual description of the concepts.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are an helpful assistant helping building an ontology."
        ),
        ChatMessage(
            role="user",
            content=f'Based on the context given, define if there is a hierarchy between the listed concepts. The result should be given as a python list of list of string with double quotes. Return only triples with "is_generalised_by" relation, without any explanation.\n\n###\nHere is an example. Concepts: animal, mammal, dog(canine), flower\n[["mammal","is_generalised_by","animal"], ["dog","is_generalised_by","mammal"], ["dog","is_generalised_by","animal"]]\n###\n\n# Context: {doc_context}\n\n{concepts_description}'
        )
    ]
    return prompt


def mistral_prompt_owl_axiom_extraction(kr_description: str, namespace: str) -> list[ChatMessage]:
    """Prompt template for axiom extraction with Mistral 7B model.

    Parameters
    ----------
    kr_description: str
        Textual description of the knowledge representation.
    namespace: str
        The name space used for axiom generation.

    Returns
    -------
    list[ChatMessage]
        Prompt template.
    """
    prompt = [
        ChatMessage(
            role="system",
            content="You are a helpful assistant in building an ontology. You are fluent in the W3C Semantic Web stack and in the RDF, RDFS, and OWL languages."
        ),
        ChatMessage(
            role="user",
            content=f'Use the following classes, individuals and relations to construct the complete OWL ontology in the Turtle format. Use the following namespace: {namespace}. Include the RDF, RDFS, and OWL prefixes. Add point at the end of all declarations. Return only the turtle file.\n\n{kr_description}'
        )
    ]
    return prompt


def add_pipeline_components(pipeline: Pipeline) -> Pipeline:
    """Create pipeline with LLM components.

    Parameters
    ----------
    pipeline: Pipeline
        The pipeline into which the components are to be added.

    Returns
    -------
    Pipeline
        The pipeline updated with new components.
    """
    mistral_generator = CustomLLMGenerator()
    llm_cterm_extraction = LLMTermExtraction(
        mistral_prompt_concept_term_extraction, mistral_generator
    )
    pipeline.add_pipeline_component(llm_cterm_extraction)

    llm_cterm_enrichment = LLMBasedTermEnrichment(
        mistral_prompt_term_enrichment, mistral_generator)
    pipeline.add_pipeline_component(llm_cterm_enrichment)

    llm_concept_extraction = LLMBasedConceptExtraction(
        mistral_prompt_concept_extraction, mistral_generator)
    pipeline.add_pipeline_component(llm_concept_extraction)

    llm_hierarchisation = LLMBasedHierarchisation(
        mistral_prompt_hierarchisation, mistral_generator)
    pipeline.add_pipeline_component(llm_hierarchisation)

    llm_rterm_extraction = LLMTermExtraction(
        mistral_prompt_relation_term_extraction, mistral_generator
    )
    pipeline.add_pipeline_component(llm_rterm_extraction)

    llm_rterm_enrichment = LLMBasedTermEnrichment(
        mistral_prompt_term_enrichment, mistral_generator)
    pipeline.add_pipeline_component(llm_rterm_enrichment)

    llm_relation_extraction = LLMBasedRelationExtraction(
        mistral_prompt_relation_extraction, mistral_generator)
    pipeline.add_pipeline_component(llm_relation_extraction)

    llm_axiom_extraction = LLMBasedOWLAxiomExtraction(
        mistral_prompt_owl_axiom_extraction,
        mistral_generator,
        "https://github.com/wikit-ai/olaf-llm-nlp4kgc2024/o/example#"
    )
    pipeline.add_pipeline_component(llm_axiom_extraction)

    return pipeline


def main() -> None:
    """LLM pipeline execution."""
    pipeline = create_pipeline()
    pipeline = add_pipeline_components(pipeline)
    pipeline.run()

    kr_serialiser = KRJSONSerialiser()
    kr_serialisation_path = os.path.join(
        os.getenv("RESULTS_PATH"), "llm_pipeline", "llm_pipeline_defect_kr_mistral.json")
    kr_serialiser.serialise(kr=pipeline.kr, file_path=kr_serialisation_path)

    kr_rdf_graph_path = os.path.join(
        os.getenv("RESULTS_PATH"), "llm_pipeline", "llm_pipeline_defect_kr_rdf_graph_mistral.ttl")
    pipeline.kr.rdf_graph.serialize(kr_rdf_graph_path, format="ttl")

    print(f"Nb concepts: {len(pipeline.kr.concepts)}")
    print(f"Nb relations: {len(pipeline.kr.relations)}")
    print(f"Nb metarelations: {len(pipeline.kr.metarelations)}")
    print(f"The KR object has been JSON serialised in : {kr_serialisation_path}")
    print(f"The KR RDF graph has been serialised in : {kr_rdf_graph_path}")


if __name__ == "__main__":
    main()
